{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoLJqXYT9I3l"
   },
   "source": [
    "## Loading Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cz5K5rJV9GZN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tq4QdTXi9e4Y",
    "outputId": "92551a59-a60e-41cc-a59b-975cd2fa6c25"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzrtj0QI9l2L"
   },
   "outputs": [],
   "source": [
    "# Load BBC dataset from Google Drive\n",
    "data_path = \"/content/drive/My Drive/bbc 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "Of5khMMD9o0l",
    "outputId": "a4e53d3f-d8e4-4ecf-e7f7-1ca8a6f42e73"
   },
   "outputs": [],
   "source": [
    "categories = os.listdir(data_path)\n",
    "data = []\n",
    "for category in categories:\n",
    "    category_path = os.path.join(data_path, category)\n",
    "    if os.path.isdir(category_path):\n",
    "        for file_name in os.listdir(category_path):\n",
    "            file_path = os.path.join(category_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                    content = f.read()\n",
    "                    data.append([category, content])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['category', 'text'])\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "RfHJLqPm94F1",
    "outputId": "19d63856-af1d-4667-f323-7e32a81ddadb"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJG2xBNu95VU",
    "outputId": "2e79dd99-2878-4709-f62d-14425afa3973"
   },
   "outputs": [],
   "source": [
    "def clean_text_pipeline(text):\n",
    "    \"\"\"\n",
    "    Cleans raw BBC-style news text.\n",
    "    - Removes HTML, line breaks, unwanted characters.\n",
    "    - Converts to lowercase.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # Replace newlines and multiple spaces\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove special characters (keep .!? for sentence structure)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?'\\\"]\", ' ', text)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply to your dataframe\n",
    "df['clean_text'] = df['text'].apply(clean_text_pipeline)\n",
    "\n",
    "# Remove short/empty rows\n",
    "df = df[df['clean_text'].str.len() > 30].reset_index(drop=True)\n",
    "\n",
    "# Show sample\n",
    "print(df[['category', 'clean_text']].head(10))\n",
    "print(f\"\\nTotal cleaned articles: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bYfuIe_79-G0",
    "outputId": "75c0eacb-eaef-4b51-90eb-528ce00c7850"
   },
   "outputs": [],
   "source": [
    "df.head(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "pgoRDJrf-CKQ",
    "outputId": "deb3d242-d67a-4c02-9d17-b71d76d191d5"
   },
   "outputs": [],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGJcD6EC-FjP"
   },
   "outputs": [],
   "source": [
    "df = df[['category', 'clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "sua-p7hs-IP7",
    "outputId": "bcc1bc3d-487a-4ede-d50b-43eec915f478"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nf6R5Zsg-Kws"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uw1neHq-OnR"
   },
   "outputs": [],
   "source": [
    "# Set style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "ieOa9pwl-RDx",
    "outputId": "ffe2496d-fd25-414c-d2b5-f2e54a630d7f"
   },
   "outputs": [],
   "source": [
    "# Count plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(\n",
    "    data=df,\n",
    "    x='category',\n",
    "    hue='category',\n",
    "    order=df['category'].value_counts().index,\n",
    "    palette='pastel',\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Article Count per Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRVdCd2m-UWU"
   },
   "outputs": [],
   "source": [
    "# Save to Drive\n",
    "#df.to_csv('/content/drive/My Drive/bbc_clean_articles.csv', index=False)\n",
    "#print(\"Cleaned dataset saved to your Google Drive as 'bbc_articles.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPsk15s8-hkG"
   },
   "source": [
    "#### Using preprocessed CSV file from previous analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35I263eN-XoJ"
   },
   "outputs": [],
   "source": [
    "# Using the Csv file to continue the Modeling\n",
    "df.to_csv(\"/content/drive/MyDrive/bbc_clean_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeEmMzK7-0LS"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksMDoB7k-3AO"
   },
   "outputs": [],
   "source": [
    "def assign_top_subcategories_v2(text, main_cat, max_cats=3):\n",
    "    text = text.lower()\n",
    "    match_counter = Counter()\n",
    "\n",
    "    # All sub-category mappings\n",
    "    all_subcats = {\n",
    "        'entertainment': {\n",
    "            'cinema': ['film', 'movie', 'actor', 'box office', 'hollywood'],\n",
    "            'music': ['singer', 'album', 'music', 'song', 'chart', 'band'],\n",
    "            'literature': ['book', 'novel', 'author', 'poem'],\n",
    "            'awards': ['oscar', 'bafta', 'grammy', 'award'],\n",
    "            'tv': ['series', 'tv', 'show', 'episode']\n",
    "\n",
    "        },\n",
    "        'tech': {\n",
    "            'internet': ['internet', 'web', 'online', 'broadband', 'website'],\n",
    "            'software': ['software', 'app', 'application'],\n",
    "            'hardware': ['device', 'gadget', 'mobile', 'laptop', 'pc', 'phone', 'processor'],\n",
    "            'ai': ['ai', 'artificial intelligence', 'machine learning', 'neural network'],\n",
    "            'cybersecurity': ['virus', 'security', 'hacker', 'phishing']\n",
    "        },\n",
    "        'business': {\n",
    "            'stock_market': ['stock', 'share', 'market', 'dow', 'nasdaq'],\n",
    "            'banking': ['bank', 'loan', 'interest rate'],\n",
    "            'mergers': ['merger', 'acquisition', 'deal', 'buyout'],\n",
    "            'economy': ['inflation', 'recession', 'gdp'],\n",
    "            'investment': ['investment', 'stock', 'share']\n",
    "        },\n",
    "        'politics': {\n",
    "            'election': ['election', 'vote', 'voting'],\n",
    "            'government': ['minister', 'government', 'policy'],\n",
    "            'parliament': ['parliament', 'mps'],\n",
    "            'foreign_policy': ['embassy', 'diplomat', 'treaty']\n",
    "\n",
    "        },\n",
    "        'sport': {\n",
    "            'football': ['football', 'soccer', 'goal', 'football team', 'laliga', 'premier league'],\n",
    "            'cricket': ['cricket', 'wicket', 'innings'],\n",
    "            'tennis': ['tennis', 'wimbledon', 'serve'],\n",
    "            'rugby': ['rugby', 'scrum'],\n",
    "            'athletics': ['athletics', 'track', 'field', 'olympics'],\n",
    "            'club manager': ['club manager', 'manager', 'coach'],\n",
    "            'player': ['player', 'athlete', 'coach'],\n",
    "            'team': ['team', 'club', 'coach']\n",
    "\n",
    "        },\n",
    "        'general': {\n",
    "            'education': ['school', 'university', 'exam', 'education'],\n",
    "            'health_science': ['health', 'science', 'disease', 'research', 'doctor'],\n",
    "            'environment': ['climate', 'environment', 'carbon', 'pollution'],\n",
    "            'social_issues': ['poverty', 'homeless', 'protest', 'racism'],\n",
    "            'transport': ['train', 'car', 'transport', 'traffic'],\n",
    "            'disasters': ['earthquake', 'flood', 'disaster', 'tsunami'],\n",
    "            'international': ['un', 'war', 'foreign', 'global', 'international'],\n",
    "            'immigration': ['immigration', 'migrant', 'asylum', 'refugee', 'visa', 'deportation'],\n",
    "            'housing': ['housing', 'rent', 'landlord', 'mortgage', 'eviction']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Merge general subcats with main category\n",
    "    if main_cat in all_subcats:\n",
    "        subcats = {**all_subcats[main_cat], **all_subcats['general']}\n",
    "    else:\n",
    "        subcats = all_subcats['general']\n",
    "\n",
    "    for subcat, keywords in subcats.items():\n",
    "        # Skip known mismatches like transport in entertainment\n",
    "        if main_cat == 'entertainment' and subcat == 'transport':\n",
    "            continue\n",
    "\n",
    "        for kw in keywords:\n",
    "            pattern = r'\\b' + re.escape(kw) + r'\\b'\n",
    "            if re.search(pattern, text):\n",
    "                match_counter[subcat] += 1\n",
    "                break  # Avoid duplicate counting\n",
    "\n",
    "    # List subcategories that don't make sense under 'entertainment'\n",
    "    irrelevant_for_entertainment = ['immigration', 'environment', 'disasters', 'housing', 'transport', 'health_science', 'education', 'literature']\n",
    "   # Remove only loosely relevant subcategories for business\n",
    "    irrelevant_for_business = ['environment', 'health_science', 'social_issues', 'disasters']\n",
    "    irrelevant_for_tech = ['transport', 'education', 'literature', 'disasters']\n",
    "    irrelevant_for_sport = ['literature', 'education', 'transport', 'housing']\n",
    "    irrelevant_for_politics = ['music', 'tv', 'cinema', 'transport', 'sports']\n",
    "\n",
    "    # Remove irrelevant matches if the main category is 'entertainment'\n",
    "    if main_cat == 'entertainment':\n",
    "        for subcat in irrelevant_for_entertainment:\n",
    "            if subcat in match_counter:\n",
    "                del match_counter[subcat]\n",
    "    # End of added logic\n",
    "    # After computing match_counter\n",
    "    if main_cat == 'business':\n",
    "        for subcat in irrelevant_for_business:\n",
    "          match_counter.pop(subcat, None)\n",
    "\n",
    "    elif main_cat == 'tech':\n",
    "          for subcat in irrelevant_for_tech:\n",
    "            match_counter.pop(subcat, None)\n",
    "\n",
    "    elif main_cat == 'sport':\n",
    "          for subcat in irrelevant_for_sport:\n",
    "            match_counter.pop(subcat, None)\n",
    "\n",
    "    elif main_cat == 'politics':\n",
    "          for subcat in irrelevant_for_politics:\n",
    "            match_counter.pop(subcat, None)\n",
    "\n",
    "\n",
    "    top_matches = [cat for cat, _ in match_counter.most_common(max_cats)]\n",
    "    return top_matches if top_matches else ['misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wa-lyhaS_DfX"
   },
   "outputs": [],
   "source": [
    "# Apply the sub-category assignment function\n",
    "df['sub_category_list'] = df.apply(\n",
    "    lambda row: assign_top_subcategories_v2(row['clean_text'], row['category']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vv3kUqVz_GY5"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates and sort for consistent formatting\n",
    "df['sub_category_list'] = df['sub_category_list'].apply(\n",
    "    lambda x: sorted(list(set([tag.strip().lower() for tag in x])))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxmudM9R_JrH"
   },
   "outputs": [],
   "source": [
    "# Remove overly generic/noisy subcategories like 'international'\n",
    "noisy_tags = {'international'}  # You can add more like 'transport' if needed\n",
    "df['sub_category_list'] = df['sub_category_list'].apply(\n",
    "    lambda x: [tag for tag in x if tag not in noisy_tags]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFVNjRNI_MOj"
   },
   "outputs": [],
   "source": [
    "# Join the cleaned list into comma-separated string\n",
    "df['sub_category'] = df['sub_category_list'].apply(lambda x: ', '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8Av6_2p_PJ-",
    "outputId": "94ada52b-5d17-4ead-aa37-df3361f259dd"
   },
   "outputs": [],
   "source": [
    "# Preview\n",
    "print(df[['category', 'sub_category', 'clean_text']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYNpbCCB_SpD",
    "outputId": "f7ea8382-405f-4166-e4ff-ab62a3ba70d3"
   },
   "outputs": [],
   "source": [
    "# Preview\n",
    "print(df[['category', 'sub_category', 'clean_text']].tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWskimn5_V-p"
   },
   "outputs": [],
   "source": [
    "# Save the full DataFrame to CSV\n",
    "#df.to_csv(\"bbc_articles_with_subcategories3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9Jisr-X_euE"
   },
   "outputs": [],
   "source": [
    "# For Google Colab, download the file\n",
    "#from google.colab import files\n",
    "#files.download(\"bbc_articles_with_subcategories3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVlGRxSt_jRl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9sGiQqMCNl4"
   },
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HP7cXxOmCXGY",
    "outputId": "8bd14acc-d414-48b5-bc66-f7efd446137d"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets scikit-learn torch matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uwJIgAsCodP"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for data processing, machine learning, and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast, DistilBertForSequenceClassification,\n",
    "    RobertaTokenizerFast, RobertaForSequenceClassification,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnAHTtTCCoKn",
    "outputId": "ff25fd11-dfaa-4228-a2f3-e6bab7818901"
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility, ensures consistent results across runs\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQEKFGPSDF-r"
   },
   "source": [
    "#####  LOAD AND EXPLORE THE PRE-CLEANED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XY1fyp9oCbGI",
    "outputId": "b1df2058-7883-4b29-f575-693078630062"
   },
   "outputs": [],
   "source": [
    "print(\"Loading pre-cleaned BBC data...\")\n",
    "# Load the CSV file containing BBC articles with pre-assigned sub-categories\n",
    "df = pd.read_csv(\"/content/drive/My Drive/bbc_articles_with_subcategories3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIm-xWYxCoBw",
    "outputId": "8e6d388a-eb9e-4545-8c7e-46e180e0f178"
   },
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(f\"Loaded {len(df)} articles\")\n",
    "print(f\"Columns in dataset: {df.columns.tolist()}\")\n",
    "print(f\"Main categories: {df['category'].unique()}\")\n",
    "print(f\"Number of unique sub-categories: {df['sub_category'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "415g8MQDDYR5",
    "outputId": "2a1913c8-9eae-4dfe-c694-d760281a2571"
   },
   "outputs": [],
   "source": [
    "# Show a sample of the data to understand its structure\n",
    "print(\"\\nSample data (first 5 rows):\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nLICW-6D4nw",
    "outputId": "841e53be-a6de-4cf3-a5ea-a6b30e102ab9"
   },
   "outputs": [],
   "source": [
    "# Check for data quality issues - missing values, etc.\n",
    "# Check for data quality issues - missing values, etc.\n",
    "print(\"\\nData quality check:\")\n",
    "print(f\"Missing values in clean_text column: {df['clean_text'].isnull().sum()}\")\n",
    "print(f\"Missing values in category column: {df['category'].isnull().sum()}\")\n",
    "print(f\"Missing values in sub_category column: {df['sub_category'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7ylDaGwmQy_"
   },
   "source": [
    "##### Checking the number of missing rows of sub-category, which is about 1.8% of the total rows, we can say it's neglegible, however, this also give room for additional sub-category; this could be revisted. One good thing is that there was no missing rows for category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlJjVILIEVuL",
    "outputId": "36c9f140-7b4f-4f97-df06-16c92d71a545"
   },
   "outputs": [],
   "source": [
    "# Display the distribution of sub-categories within each main category\n",
    "print(\"\\nSub-category distribution by category:\")\n",
    "for category in df['category'].unique():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    sub_counts = df[df['category'] == category]['sub_category'].value_counts()\n",
    "    for sub_cat, count in sub_counts.items():\n",
    "        print(f\"  {sub_cat}: {count} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13BMoAZGEtTS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gZwyssbE3zP"
   },
   "source": [
    "##### VISUALIZE SUB-CATEGORY DISTRIBUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "id": "Ebf7msIaE6XV",
    "outputId": "4a6407cf-1693-4c20-8c77-07609ac2f473"
   },
   "outputs": [],
   "source": [
    "def plot_subcategory_distributions(df):\n",
    "    \"\"\"\n",
    "    Create bar charts showing the distribution of sub-categories for each main category.\n",
    "    This helps understand the data balance and identify potential class imbalance issues.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing the BBC articles with categories and sub-categories\n",
    "    \"\"\"\n",
    "    # Create a 2x3 subplot grid to display all categories\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    axes = axes.ravel()  # Flatten the axes array for easier indexing\n",
    "\n",
    "    # Plot each category's sub-category distribution\n",
    "    for i, category in enumerate(df['category'].unique()):\n",
    "        # Filter data for current category\n",
    "        df_cat = df[df['category'] == category]\n",
    "        counts = df_cat['sub_category'].value_counts()\n",
    "\n",
    "        # Create bar chart\n",
    "        axes[i].bar(range(len(counts)), counts.values, color='#36A2EB')\n",
    "        axes[i].set_title(f'{category.capitalize()} Sub-categories')\n",
    "        axes[i].set_xlabel('Sub-category')\n",
    "        axes[i].set_ylabel('Number of Articles')\n",
    "        axes[i].set_xticks(range(len(counts)))\n",
    "        axes[i].set_xticklabels(counts.index, rotation=45, ha='right')\n",
    "\n",
    "    # Hide empty subplot if we have fewer than 6 categories\n",
    "    if len(df['category'].unique()) < 6:\n",
    "        axes[-1].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('subcategory_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Generate the visualization\n",
    "plot_subcategory_distributions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jbHOTV8FL0i"
   },
   "source": [
    "### PREPARE DATA FOR HUGGING FACE MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GN5Bi5pFHVx",
    "outputId": "09fd96ad-4e60-47dc-e6dc-b702f90d1cc5"
   },
   "outputs": [],
   "source": [
    "print(\"\\nPreparing data for modeling...\")\n",
    "\n",
    "# Convert sub-categories to list format for MultiLabelBinarizer\n",
    "# Based on the preprocessing, sub_category contains comma-separated values\n",
    "# We need to split them and convert to list format\n",
    "# Using str() to handle any non-string values and avoid split() errors\n",
    "df['sub_category_list'] = df['sub_category'].apply(lambda x: [s.strip() for s in str(x).split(',') if s.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CREs8qk9GWto"
   },
   "outputs": [],
   "source": [
    "# Create MultiLabelBinarizer to convert sub_categories to binary format\n",
    "# This transforms categorical labels into a binary matrix where each column represents a sub_category\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(df['sub_category_list'])\n",
    "subcat_classes = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_O7S5yHHKUJ",
    "outputId": "e5cfddad-e963-4238-e001-94f27cf292c1"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of unique sub-categories: {len(subcat_classes)}\")\n",
    "print(f\"Sub-categories: {subcat_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBVnS3QzHPSI"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "# Using clean_text column from the preprocessing pipeline for better results\n",
    "# Using stratification to ensure each category is represented proportionally in both sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['clean_text'], labels, test_size=0.2, random_state=42, stratify=df['category']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PD3t7k_CHeJN",
    "outputId": "efc38927-5dd3-468c-d806-5a2861c73e45"
   },
   "outputs": [],
   "source": [
    "print(f\"Training set size: {len(X_train)} articles\")\n",
    "print(f\"Validation set size: {len(X_val)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9D9vHx3Hi8z"
   },
   "source": [
    "### LOAD HUGGING FACE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "590Jg-kiHgzc",
    "outputId": "48b440de-4e62-4f04-c195-f60d405ea9b2"
   },
   "outputs": [],
   "source": [
    "# Choose RoBERTa for better accuracy in sub-category classification\n",
    "model_name = \"roberta-base\"  # Better accuracy - recommended for production\n",
    "\n",
    "print(f\"\\nUsing model: {model_name}\")\n",
    "print(\"Note: RoBERTa typically provides better accuracy than DistilBERT but takes longer to train\")"
   ]
  },
  
    "id": "EAvXUY_CH0oJ",
    "outputId": "e565b84a-689c-420f-b99a-7889e2e7fddf"
   },
   "outputs": [],
   "source": [
    "# Load RoBERTa tokenizer and model\n",
    "# RoBERTa: Improved version of BERT with better performance for text classification\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(subcat_classes),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBwNNJA6IEWZ"
   },
   "source": [
    "#### TOKENIZE THE TEXT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTGaYyUMH7nW",
    "outputId": "7f89b932-cb17-4967-f98d-6f9afdf2bb6c"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(texts):\n",
    "    \"\"\"\n",
    "    Tokenize a list of texts using the Hugging Face tokenizer.\n",
    "\n",
    "    Args:\n",
    "        texts: List of text strings to tokenize\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing tokenized inputs (input_ids, attention_mask)\n",
    "    \"\"\"\n",
    "    return tokenizer(list(texts), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "print(\"Tokenizing data...\")\n",
    "# Convert text to tokens that the model can understand\n",
    "train_encodings = tokenize_function(X_train)\n",
    "val_encodings = tokenize_function(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iFqrULSIa0e"
   },
   "source": [
    "#### CREATE CUSTOM DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUr-o1pXIU1h"
   },
   "outputs": [],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset class for BBC news articles.\n",
    "    This class handles the conversion of tokenized data and labels into PyTorch tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with tokenized encodings and labels.\n",
    "\n",
    "        Args:\n",
    "            encodings: Tokenized text data from the tokenizer\n",
    "            labels: Binary labels for sub-categories\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx: Index of the sample to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing input_ids, attention_mask, and labels as tensors\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create dataset objects for training and validation\n",
    "train_dataset = NewsDataset(train_encodings, y_train)\n",
    "val_dataset = NewsDataset(val_encodings, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaaBsiNWItbr"
   },
   "source": [
    "#### CONFIGURE TRAINING ARGUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvGWxNNNIrHv"
   },
   "outputs": [],
   "source": [
    "# Define training arguments for the Hugging Face Trainer with checkpointing\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./{model_name.replace(\"/\", \"_\")}_results',  # Directory to save model outputs\n",
    "    per_device_train_batch_size=8,                           # Batch size for training\n",
    "    per_device_eval_batch_size=8,                            # Batch size for evaluation\n",
    "    num_train_epochs=8,                                     # Set to 10 epochs for better performance\n",
    "    eval_strategy=\"epoch\",                                   # Evaluate after each epoch (updated parameter name)\n",
    "    save_strategy=\"epoch\",                                   # Save model after each epoch\n",
    "    save_total_limit=3,                                      # Keep only last 3 checkpoints to save space\n",
    "    logging_dir=f'./{model_name.replace(\"/\", \"_\")}_logs',   # Directory for training logs\n",
    "    logging_steps=10,                                        # Log every 10 steps\n",
    "    load_best_model_at_end=True,                            # Load the best model at the end\n",
    "    metric_for_best_model=\"eval_loss\",                      # Use validation loss to determine best model\n",
    "    warmup_steps=500,                                        # Number of warmup steps for learning rate\n",
    "    weight_decay=0.01,                                       # Weight decay for regularization\n",
    "    learning_rate=2e-5,                                      # Learning rate for optimization\n",
    "    # Checkpointing settings for crash recovery\n",
    "    save_steps=100,                                          # Save every 100 steps (additional checkpoints)\n",
    "    dataloader_pin_memory=False,                             # Reduce memory usage\n",
    "    dataloader_num_workers=0,                                # Reduce memory usage\n",
    "    remove_unused_columns=False,                             # Keep all columns for resuming\n",
    "    # Disable Weights & Biases logging\n",
    "    report_to=[],                                            # Disable all reporting (wandb, tensorboard, etc.)\n",
    "    logging_strategy=\"steps\"                                 # Log to console only\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOOPeBTgLF2O"
   },
   "source": [
    "#### DEFINE EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOEX14AWLDNB"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the model predictions.\n",
    "\n",
    "    Args:\n",
    "        pred: Prediction object containing predictions and label_ids\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing precision, recall, and F1-score\n",
    "    \"\"\"\n",
    "    # Convert logits to binary predictions using threshold 0.5\n",
    "    preds = pred.predictions > 0.5\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    return {\n",
    "        'precision': precision_score(labels, preds, average='micro'),\n",
    "        'recall': recall_score(labels, preds, average='micro'),\n",
    "        'f1': f1_score(labels, preds, average='micro'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPzYpMI9LPo8"
   },
   "source": [
    "#### INITIALIZE AND TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWDWUA4-LNul"
   },
   "outputs": [],
   "source": [
    "# Create the Trainer object that handles training, evaluation, and prediction\n",
    "trainer = Trainer(\n",
    "    model=model,                    # The model to train\n",
    "    args=training_args,             # Training arguments\n",
    "    train_dataset=train_dataset,    # Training data\n",
    "    eval_dataset=val_dataset,       # Validation data\n",
    "    compute_metrics=compute_metrics # Function to compute metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "2-xvmYinMa2U",
    "outputId": "6da402b4-aee5-4d99-82b3-f48c260e7673"
   },
   "outputs": [],
   "source": [
    "# Training for 8 epochs with 2230 samples (~17,840 total steps)\n",
    "# WARNING: Training beyond 8 epochs may cause overfitting due to small dataset size\n",
    "# Monitor validation loss,\n",
    "# stop early if it increases while training loss decreases\n",
    "# Start the training process with crash recovery capability\n",
    "print(f\"\\nStarting {model_name} training...\")\n",
    "print(\"This may take several minutes depending on your hardware and dataset size.\")\n",
    "print(\"Training will automatically resume from the last checkpoint if interrupted.\")\n",
    "\n",
    "# Check if there's a previous checkpoint to resume from\n",
    "checkpoint_dir = f'./{model_name.replace(\"/\", \"_\")}_results'\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    # Find the latest checkpoint\n",
    "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "        print(f\"Found existing checkpoint: {checkpoint_path}\")\n",
    "        print(\"Resuming training from checkpoint...\")\n",
    "        print(\"Starting training now... (this may take a moment to initialize)\")\n",
    "        trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting fresh training...\")\n",
    "        print(\"Starting training now... (this may take a moment to initialize)\")\n",
    "        trainer.train()\n",
    "else:\n",
    "    print(\"No previous training found, starting fresh training...\")\n",
    "    print(\"Starting training now... (this may take a moment to initialize)\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKhmPiwyNlL7",
    "outputId": "52f1c15a-9f40-4332-e607-8a1a3f01023f"
   },
   "outputs": [],
   "source": [
    "# Save the trained model, tokenizer, and label binarizer for future use\n",
    "model_save_path = f\"./{model_name.replace('/', '_')}_final_model\"\n",
    "trainer.save_model(model_save_path)  # Saves model weights and config\n",
    "tokenizer.save_pretrained(model_save_path)  # Saves tokenizer config/vocab\n",
    "\n",
    "# Save the MultiLabelBinarizer (for decoding predictions later)\n",
    "import pickle\n",
    "with open(f\"{model_save_path}/mlb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mlb, f)\n",
    "\n",
    "print(f\"\\nModel, tokenizer, and label binarizer saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTnc_7T1NvZ6"
   },
   "outputs": [],
   "source": [
    "# from transformers import RobertaForSequenceClassification, RobertaTokenizerFast\n",
    "# import pickle\n",
    "\n",
    "# Path to your saved model\n",
    "# model_save_path = \"./roberta_base_final_model\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "# model = RobertaForSequenceClassification.from_pretrained(model_save_path)\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained(model_save_path)\n",
    "\n",
    "# Load MultiLabelBinarizer\n",
    "# with open(f\"{model_save_path}/mlb.pkl\", \"rb\") as f:\n",
    "    #mlb = pickle.load(f)\n",
    "\n",
    "# Now you can use model, tokenizer, and mlb for predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvQP5ChYXBFU"
   },
   "source": [
    "#### EVALUATE THE TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "A-d7O3zJW-cK",
    "outputId": "9ed8bea1-bc81-45be-80e2-0f7f11f6eafb"
   },
   "outputs": [],
   "source": [
    "print(f\"\\nEvaluating {model_name} model...\")\n",
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai7Nt2ysXu6c"
   },
   "source": [
    "#### MAKE PREDICTIONS AND ANALYZE RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfL7Ad46Y9kI"
   },
   "source": [
    "##### The BBC sub-category classifier achieved 91% precision and 39% recall, meaning it's highly accurate when it makes predictions but misses many sub-categories. The model excels at identifying specific topics like cinema (90% F1) and government (90% F1), but struggles with categories like banking, economy, and education (0% performance). This conservative approach prioritizes accuracy over coverage - when the model predicts a sub-category, you can trust it 91% of the time, though it only identifies 39% of all actual sub-categories in the articles. The performance suggests the model works best for applications where high confidence predictions are more valuable than comprehensive coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-QPdgOkFXgp6",
    "outputId": "b657d24c-1524-4811-a892-6db8427dd119"
   },
   "outputs": [],
   "source": [
    "print(\"\\nMaking predictions on validation set...\")\n",
    "# Generate predictions for the validation set\n",
    "predictions = trainer.predict(val_dataset)\n",
    "preds = (predictions.predictions > 0.5).astype(int)\n",
    "decoded_preds = mlb.inverse_transform(preds)\n",
    "\n",
    "# Generate detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"This report shows precision, recall, and F1-score for each sub-category:\")\n",
    "print(classification_report(y_val, preds, target_names=subcat_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28TAIt0KZflz"
   },
   "source": [
    "#### CREATE CONFUSION MATRICES FOR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "BIhjv69kaKUv",
    "outputId": "8ed5906c-2d89-4c68-df1b-b80f2c0c83d1"
   },
   "outputs": [],
   "source": [
    "def plot_category_confusion_matrix(df, y_true, y_pred, subcat_classes, X_val):\n",
    "    \"\"\"\n",
    "    Create confusion matrix for main categories (business, entertainment, politics, sport, tech).\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with original data\n",
    "        y_true: True sub-category labels\n",
    "        y_pred: Predicted sub-category labels\n",
    "        subcat_classes: Names of the sub-categories\n",
    "        X_val: Validation data (Series of clean_text)\n",
    "    \"\"\"\n",
    "    # Get the main categories for validation set using the index of X_val\n",
    "    val_categories = df.loc[X_val.index]['category'].values\n",
    "\n",
    "    # Create a mapping from sub-categories to main categories\n",
    "    subcat_to_category = {}\n",
    "    # Iterate through the original DataFrame to build the mapping\n",
    "    for index, row in df.iterrows():\n",
    "        # Ensure sub_category is not NaN and is a string before splitting\n",
    "        if pd.notna(row['sub_category']):\n",
    "            subcats = [s.strip() for s in str(row['sub_category']).split(',') if s.strip()]\n",
    "            for subcat in subcats:\n",
    "                if subcat in subcat_classes:\n",
    "                    subcat_to_category[subcat] = row['category']\n",
    "\n",
    "    # Convert sub-category predictions to main category predictions\n",
    "    true_categories = []\n",
    "    pred_categories = []\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        # Get true main category\n",
    "        true_subcats = [subcat_classes[j] for j in range(len(y_true[i])) if y_true[i][j] == 1]\n",
    "        # Determine the true main category based on the original DataFrame's category\n",
    "        true_cat = df.loc[X_val.index[i], 'category']\n",
    "\n",
    "\n",
    "        # Get predicted main category - find the first predicted subcategory that has a mapping\n",
    "        pred_cat = val_categories[i] # Default to the true main category if no predicted subcat is found\n",
    "        pred_subcats = [subcat_classes[j] for j in range(len(y_pred[i])) if y_pred[i][j] == 1]\n",
    "        for subcat in pred_subcats:\n",
    "             if subcat in subcat_to_category:\n",
    "                pred_cat = subcat_to_category[subcat]\n",
    "                break # Use the first predicted subcategory's main category\n",
    "\n",
    "        true_categories.append(true_cat)\n",
    "        pred_categories.append(pred_cat)\n",
    "\n",
    "    # Create confusion matrix for main categories\n",
    "    main_categories = sorted(df['category'].unique().tolist()) # Get unique categories from the original df\n",
    "    cm = confusion_matrix(true_categories, pred_categories, labels=main_categories)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=main_categories, yticklabels=main_categories)\n",
    "    plt.title('Confusion Matrix - Main Categories')\n",
    "    plt.xlabel('Predicted Category')\n",
    "    plt.ylabel('True Category')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('category_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Print category-wise metrics\n",
    "    print(\"\\nCategory-wise Performance:\")\n",
    "    print(classification_report(true_categories, pred_categories, labels=main_categories))\n",
    "\n",
    "# Generate confusion matrix for main categories\n",
    "plot_category_confusion_matrix(df, y_val, preds, subcat_classes, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pafrbmBmbMYI"
   },
   "source": [
    "#### SAVE THE TRAINED MODEL AND COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kuZ8jUkbBZZ",
    "outputId": "21c24cfa-c642-4ad3-e35b-02a5a262c8e7"
   },
   "outputs": [],
   "source": [
    "# Save the trained model, tokenizer, and label binarizer for future use\n",
    "model_save_path = f\"./{model_name.replace('/', '_')}_final_model\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "with open(f\"{model_save_path}/mlb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mlb, f)\n",
    "\n",
    "print(f\"\\nModel and components saved to: {model_save_path}\")\n",
    "print(\"This includes:\")\n",
    "print(\"- The trained model weights\")\n",
    "print(\"- The tokenizer\")\n",
    "print(\"- The label binarizer (mlb.pkl)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wKUCy6fbZam"
   },
   "source": [
    "#### CREATE PREDICTION FUNCTION FOR NEW TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoICiZYlbY6L"
   },
   "outputs": [],
   "source": [
    "def predict_subcategory(text, trainer, mlb, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict sub-category for a new text using the trained model.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to classify\n",
    "        trainer: Trained Hugging Face trainer object\n",
    "        mlb: MultiLabelBinarizer used for label encoding\n",
    "        tokenizer: Tokenizer used for text preprocessing\n",
    "\n",
    "    Returns:\n",
    "        List of predicted sub-categories\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, truncation=True, padding='max_length', max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "    # Move input tensors to the same device as the model\n",
    "    inputs = {name: tensor.to(trainer.model.device) for name, tensor in inputs.items()}\n",
    "\n",
    "    # Make prediction without computing gradients (faster inference)\n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.model(**inputs)\n",
    "        predictions = torch.sigmoid(outputs.logits)  # Apply sigmoid to get probabilities\n",
    "        preds = (predictions > 0.5).int()  # Convert to binary predictions\n",
    "\n",
    "    # Decode predictions back to sub-category names\n",
    "    # Move predictions back to CPU before decoding with mlb\n",
    "    decoded_preds = mlb.inverse_transform(preds.cpu().numpy())\n",
    "    return decoded_preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uNPEU4ab8VK"
   },
   "source": [
    "#### TEST THE MODEL WITH EXAMPLE TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpUlKtAkdDmN",
    "outputId": "1ec03024-1f86-467e-be17-5645bf9e29fb"
   },
   "outputs": [],
   "source": [
    "print(\"\\nTesting the model with example texts:\")\n",
    "# Sample texts that the model can predict based on training data\n",
    "sample_texts = [\n",
    "    \"The government announced new policies to address climate change.\",\n",
    "    \"Microsoft released a new software update with enhanced security features.\",\n",
    "    \"The parliament debated new legislation on healthcare reform.\",\n",
    "    \"The stock market showed strong gains in technology sector.\"\n",
    "]\n",
    "\n",
    "# Make predictions for each sample text\n",
    "for text in sample_texts:\n",
    "    prediction = predict_subcategory(text, trainer, mlb, tokenizer)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Predicted sub-category: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYRviZgdaYB"
   },
   "source": [
    "#### GENERATE FINAL PERFORMANCE SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JijafqbdSVz",
    "outputId": "d301b527-c20b-493e-a329-5d3e9d2e65be"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model used: {model_name}\")\n",
    "print(f\"Total articles in dataset: {len(df)}\")\n",
    "print(f\"Number of main categories: {len(df['category'].unique())}\")\n",
    "print(f\"Number of sub-categories: {len(subcat_classes)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Precision: {eval_results.get('eval_precision', 0):.4f}\")\n",
    "print(f\"  Recall: {eval_results.get('eval_recall', 0):.4f}\")\n",
    "print(f\"  F1-Score: {eval_results.get('eval_f1', 0):.4f}\")\n",
    "print(f\"  Loss: {eval_results.get('eval_loss', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh3wMn0CeBsW"
   },
   "source": [
    "#### SAVE RESULTS SUMMARY FOR FUTURE REFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIEsz1ileHqq"
   },
   "outputs": [],
   "source": [
    "# Create a comprehensive summary of the modeling results\n",
    "results_summary = {\n",
    "    'model_name': model_name,\n",
    "    'total_articles': len(df),\n",
    "    'categories': df['category'].unique().tolist(),\n",
    "    'sub_categories': subcat_classes.tolist(),\n",
    "    'training_samples': len(X_train),\n",
    "    'validation_samples': len(X_val),\n",
    "    'performance_metrics': eval_results,\n",
    "    'model_save_path': model_save_path\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab7pN2wueL7V",
    "outputId": "6fa2d9f2-ee6f-468d-b7ab-ca9b1898735d"
   },
   "outputs": [],
   "source": [
    "# Save the summary to Google Drive\n",
    "with open(\"/content/drive/My Drive/bbc_modeling_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results_summary, f)\n",
    "\n",
    "print(f\"\\nResults summary saved to: /content/drive/My Drive/bbc_modeling_results.pkl\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODELING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"- {model_save_path}/ (trained model and components)\")\n",
    "print(\"- subcategory_distributions.png (data visualization)\")\n",
    "print(\"- confusion_matrices.png (performance analysis)\")\n",
    "print(\"- /content/drive/My Drive/bbc_modeling_results.pkl (results summary)\")\n",
    "print(\"\\nYou can now use the predict_subcategory() function to classify new texts!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
